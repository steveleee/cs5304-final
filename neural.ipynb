{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/paperspace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/paperspace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "USE_CUDA = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    USE_CUDA = True\n",
    "\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path, word2idx, embedding_dim=100):\n",
    "    with open(path) as f:\n",
    "        embeddings = np.zeros((len(word2idx), embedding_dim))\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            index = word2idx.get(word)\n",
    "            if index:\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[index] = vector\n",
    "        return torch.from_numpy(embeddings).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(path):\n",
    "    \"\"\"\n",
    "    creates a dictionary mapping words to vectors from a file in glove format.\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        glove = {}\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            glove[word] = vector\n",
    "        return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_path = 'data/glove/glove.6B.300d.txt'\n",
    "embedding_path = 'data/GoogleNews-vectors-negative300.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time glove = load_glove(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/winemag-data-130k-v2.csv\")\n",
    "raw_descriptions = raw_data['description']\n",
    "raw_varieties = raw_data['variety']\n",
    "raw_provinces = raw_data['province']\n",
    "raw_points = raw_data['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pinot noir': 0, 'chardonnay': 1, 'cabernet sauvignon': 2, 'riesling': 3, 'sauvignon blanc': 4, 'syrah': 5, 'rosé': 6, 'merlot': 7, 'nebbiolo': 8, 'zinfandel': 9, 'sangiovese': 10, 'malbec': 11}\n"
     ]
    }
   ],
   "source": [
    "valid_varieties = set(['pinot noir', 'chardonnay', 'cabernet sauvignon', 'riesling', 'sauvignon blanc', 'syrah', 'rosé', 'merlot', 'nebbiolo', 'zinfandel', 'sangiovese', 'malbec']) #, 'portuguese red', 'white blend', 'sparkling blend', 'tempranillo', 'rhône-style red blend', 'pinot gris', 'champagne blend', 'cabernet franc', 'grüner veltliner', 'portuguese white', 'bordeaux-style white blend', 'pinot grigio', 'gamay', 'gewürztraminer', 'viognier', 'shiraz'])\n",
    "excluded_words = set(['pinot', 'noir', 'chardonnay', 'cabernet', 'sauvignon', 'bordeaux-style', 'blend', 'riesling', 'sauvignon',  'blanc', 'syrah', 'rosé', 'merlot', 'nebbiolo', 'zinfandel', 'sangiovese', 'malbec', 'portuguese', 'tempranillo', 'rhône-style', 'pinot', 'gris', 'champagne', 'franc', 'grüner',  'veltliner', 'portuguese', 'grigio', 'gamay', 'gewürztraminer', 'viognier', 'shiraz', 'flavor', 'wine'])\n",
    "\n",
    "varieties = ['pinot noir', 'chardonnay', 'cabernet sauvignon', 'riesling', 'sauvignon blanc', 'syrah', 'rosé', 'merlot', 'nebbiolo', 'zinfandel', 'sangiovese', 'malbec']\n",
    "label_to_idx = {word: idx for idx, word in enumerate(varieties)}\n",
    "print(label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66338 66338\n",
      "[(0, 13272), (1, 11753), (2, 9472), (3, 5189), (4, 4967), (5, 4142), (6, 3564), (7, 3102), (8, 2804), (9, 2714), (10, 2707), (11, 2652)]\n"
     ]
    }
   ],
   "source": [
    "# Extract rows with just the valid varieties\n",
    "\n",
    "def process_description(des):\n",
    "    processed_description = []\n",
    "    \n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    des = des.translate(table)\n",
    "    \n",
    "    for word in des.split():\n",
    "        word = word.lower()\n",
    "        if word not in excluded_words:\n",
    "            processed_description.append(word)\n",
    "            \n",
    "    return processed_description\n",
    "\n",
    "data, labels = [], []\n",
    "\n",
    "for i, variety in enumerate(raw_varieties):\n",
    "    if type(variety) is not float:\n",
    "        variety = variety.lower()\n",
    "        if variety in valid_varieties:\n",
    "            if type(raw_descriptions[i]) is not float:                \n",
    "                data.append(process_description(raw_descriptions[i]))\n",
    "#                 if variety == 'bordeaux-style red blend' or variety == 'rhône-style red blend':\n",
    "#                     variety = 'red blend'\n",
    "                labels.append(label_to_idx[variety])\n",
    "\n",
    "print(len(data), len(labels))\n",
    "print(Counter(labels).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pineapple', 'rind', 'lemon', 'pith', 'and', 'orange', 'blossom', 'start', 'off', 'the', 'aromas', 'the', 'palate', 'is', 'a', 'bit', 'more', 'opulent', 'with', 'notes', 'of', 'honeydrizzled', 'guava', 'and', 'mango', 'giving', 'way', 'to', 'a', 'slightly', 'astringent', 'semidry', 'finish'], ['much', 'like', 'the', 'regular', 'bottling', 'from', '2012', 'this', 'comes', 'across', 'as', 'rather', 'rough', 'and', 'tannic', 'with', 'rustic', 'earthy', 'herbal', 'characteristics', 'nonetheless', 'if', 'you', 'think', 'of', 'it', 'as', 'a', 'pleasantly', 'unfussy', 'country', 'its', 'a', 'good', 'companion', 'to', 'a', 'hearty', 'winter', 'stew'], ['soft', 'supple', 'plum', 'envelopes', 'an', 'oaky', 'structure', 'in', 'this', 'supported', 'by', '15', 'coffee', 'and', 'chocolate', 'complete', 'the', 'picture', 'finishing', 'strong', 'at', 'the', 'end', 'resulting', 'in', 'a', 'valuepriced', 'of', 'attractive', 'and', 'immediate', 'accessibility'], ['slightly', 'reduced', 'this', 'offers', 'a', 'chalky', 'tannic', 'backbone', 'to', 'an', 'otherwise', 'juicy', 'explosion', 'of', 'rich', 'black', 'cherry', 'the', 'whole', 'accented', 'throughout', 'by', 'firm', 'oak', 'and', 'cigar', 'box'], ['building', 'on', '150', 'years', 'and', 'six', 'generations', 'of', 'winemaking', 'tradition', 'the', 'winery', 'trends', 'toward', 'a', 'leaner', 'style', 'with', 'the', 'classic', 'california', 'buttercream', 'aroma', 'cut', 'by', 'tart', 'green', 'apple', 'in', 'this', 'good', 'everyday', 'sipping', 'flavors', 'that', 'range', 'from', 'pear', 'to', 'barely', 'ripe', 'pineapple', 'prove', 'approachable', 'but', 'not', 'distinctive']]\n"
     ]
    }
   ],
   "source": [
    "# Print a sample of the data\n",
    "\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53070,) (53070,)\n",
      "(13268,) (13268,)\n"
     ]
    }
   ],
   "source": [
    "# Split 80/20 training-test\n",
    "\n",
    "stacked = np.hstack([np.array(data).reshape(-1, 1), np.array(labels).reshape(-1, 1)])\n",
    "np.random.shuffle(stacked)\n",
    "\n",
    "train_split = int(len(stacked) * 0.8)\n",
    "\n",
    "train_data = stacked[:train_split, :1].reshape(-1,)\n",
    "train_labels = np.array(stacked[:train_split, 1:].reshape(-1,), dtype=np.int32)\n",
    "\n",
    "test_data = stacked[train_split:, :1].reshape(-1,)\n",
    "test_labels = np.array(stacked[train_split:, 1:].reshape(-1,), dtype= np.int32)\n",
    "\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "POS = {\n",
    "    'v': 'verb', 'a': 'adjective', 's': 'satellite adjective', \n",
    "    'n': 'noun', 'r': 'adverb'}\n",
    "\n",
    "def synonym(word, pos):\n",
    "    syns = word\n",
    "    synset = None\n",
    "    if pos == 'NN':\n",
    "        synset = wn.synsets(word, 'n')\n",
    "    elif pos == 'VB':\n",
    "        synset = wn.synsets(word, 'v')\n",
    "    elif pos == 'JJ':\n",
    "        synset = wn.synsets(word, 'a')\n",
    "        \n",
    "    if synset and len(synset) > 0:\n",
    "        synset = synset[0]\n",
    "        syns = [n.replace('_', ' ') for n in synset.lemma_names()]\n",
    "        \n",
    "        if word in syns:\n",
    "            syns.remove(word)\n",
    "\n",
    "        if len(syns) > 0:\n",
    "            return random.choice(syns).split(' ')\n",
    "        \n",
    "    return [word]\n",
    "\n",
    "def augment_data(data, labels, labels_to_augment):\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "    \n",
    "    for i, des in enumerate(data):\n",
    "        if labels[i] in labels_to_augment:\n",
    "            pos = nltk.pos_tag(des)\n",
    "            new_des = []\n",
    "\n",
    "            # Replace every possible word in the description with a synonym, if possible\n",
    "            for j in range(0, len(des)):\n",
    "                new_des += synonym(des[j], pos[j][1])\n",
    "            \n",
    "            new_data.append(new_des)\n",
    "            new_labels.append(labels[i])\n",
    "    \n",
    "    return new_data, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synonym_data, synonym_labels = augment_data(train_data, train_labels, range(3, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.concatenate((train_data, synonym_data), axis=0)\n",
    "# train_labels = np.concatenate((train_labels, synonym_labels), axis=0)\n",
    "\n",
    "# train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False, stop_words='english', token_pattern='[a-z]+', ngram_range=(1, 1))\n",
    "count_vectorizer.fit(train_data)\n",
    "vocab = count_vectorizer.vocabulary_.keys()\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)} # create word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_sequence(data):\n",
    "    max_len = 0\n",
    "    \n",
    "    for seq in data:\n",
    "        if len(seq) > max_len:\n",
    "            max_len = len(seq)\n",
    "    \n",
    "    return max_len\n",
    "\n",
    "def pad_with_max_length(max_len, data):\n",
    "    res = np.zeros((len(data), max_len))\n",
    "    for i, row in enumerate(data):\n",
    "        for j, num in enumerate(row):\n",
    "            res[i, j] = num\n",
    "    return np.array(res, dtype=np.int64)\n",
    "\n",
    "def convert_to_embedding_vocab(data):\n",
    "    res = []\n",
    "    for des in data:\n",
    "        converted = []\n",
    "        for word in des:\n",
    "            if word in word2idx:\n",
    "                converted.append(word2idx[word])\n",
    "                \n",
    "        res.append(np.array(converted, dtype=np.int64))\n",
    "        \n",
    "    res = np.array(res)\n",
    "    \n",
    "    max_len = longest_sequence(res)\n",
    "    \n",
    "    return pad_with_max_length(max_len, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_embedded = convert_to_embedding_vocab(train_data)\n",
    "train_data = train_data_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "train_data_resampled, train_labels_resampled = ros.fit_sample(train_data_embedded, train_labels)\n",
    "train_data = train_data_resampled.astype('int64')\n",
    "train_labels = train_labels_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.from_numpy(train_data)\n",
    "train_labels = torch.from_numpy(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = convert_to_embedding_vocab(test_data)\n",
    "test_data = torch.from_numpy(test_data)\n",
    "test_labels = torch.from_numpy(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = load_glove_embeddings(embedding_path, word2idx, embedding_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings, num_outputs, kernel_num, kernel_sizes, static):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.static = static\n",
    "        \n",
    "        V = embeddings.shape[0]\n",
    "        D = embeddings.shape[1]\n",
    "        C = num_outputs\n",
    "        Ci = 1\n",
    "        Co = kernel_num\n",
    "        Ks = kernel_sizes\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        '''\n",
    "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
    "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
    "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
    "        '''\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (N, W, D)\n",
    "        \n",
    "        if self.static:\n",
    "            x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        '''\n",
    "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
    "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
    "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
    "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
    "        '''\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        logit = self.fc1(x)  # (N, C)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, testloader, optimizer, epochs=10, scheduler=None):\n",
    "    print(\"Start training\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        print(\"Start epoch: \" + str(epoch + 1))\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total_batches = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            if USE_CUDA:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            total_batches += 1\n",
    "\n",
    "        print('[%d, %5d] loss: %.7f' %\n",
    "                      (epoch + 1, i + 1, running_loss / total_batches))\n",
    "\n",
    "        val_loss = validate(model, testloader)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for data in testloader:\n",
    "        descriptions, labels = data\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            descriptions, labels = descriptions.cuda(), labels.cuda()\n",
    "            \n",
    "        outputs = model(Variable(descriptions))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy: %f%%' % (\n",
    "        100 * float(correct) / total))\n",
    "    \n",
    "    return float(correct) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 10649), (1, 10649), (11, 10649), (0, 10649), (6, 10649), (3, 10649), (7, 10649), (2, 10649), (9, 10649), (4, 10649), (10, 10649), (8, 10649)]\n"
     ]
    }
   ],
   "source": [
    "class_weights = np.zeros(len(label_to_idx))\n",
    "class_counter = Counter(train_labels).most_common()\n",
    "print(class_counter)\n",
    "\n",
    "for label, ct in class_counter:\n",
    "    class_weights[label] = float(ct) / class_counter[0][1]\n",
    "    \n",
    "class_weights = torch.from_numpy(class_weights.astype(np.float32))\n",
    "\n",
    "if USE_CUDA:\n",
    "    class_weights = class_weights.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Start epoch: 1\n",
      "[1,  1278] loss: 1.2774162\n",
      "Accuracy: 61.817908%\n",
      "Start epoch: 2\n",
      "[2,  1278] loss: 0.4519409\n",
      "Accuracy: 69.264396%\n",
      "Start epoch: 3\n",
      "[3,  1278] loss: 0.1504534\n",
      "Accuracy: 70.711486%\n",
      "Start epoch: 4\n",
      "[4,  1278] loss: 0.0449013\n",
      "Accuracy: 70.560748%\n",
      "Start epoch: 5\n",
      "[5,  1278] loss: 0.0109693\n",
      "Accuracy: 71.533012%\n",
      "Start epoch: 6\n",
      "[6,  1278] loss: 0.0035853\n",
      "Accuracy: 71.826952%\n",
      "Start epoch: 7\n",
      "[7,  1278] loss: 0.0028958\n",
      "Accuracy: 71.774194%\n",
      "Start epoch: 8\n",
      "[8,  1278] loss: 0.0026631\n",
      "Accuracy: 71.932469%\n",
      "Start epoch: 9\n",
      "[9,  1278] loss: 0.0019512\n",
      "Accuracy: 71.766657%\n",
      "Start epoch: 10\n",
      "[10,  1278] loss: 0.0021344\n",
      "Accuracy: 71.713898%\n",
      "Start epoch: 11\n",
      "[11,  1278] loss: 0.0019246\n",
      "Accuracy: 71.646066%\n",
      "Start epoch: 12\n",
      "[12,  1278] loss: 0.0015050\n",
      "Accuracy: 71.872174%\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Start epoch: 13\n",
      "[13,  1278] loss: 0.0017351\n",
      "Accuracy: 71.751583%\n",
      "Start epoch: 14\n",
      "[14,  1278] loss: 0.0009091\n",
      "Accuracy: 71.857100%\n",
      "Start epoch: 15\n",
      "[15,  1278] loss: 0.0007486\n",
      "Accuracy: 71.894784%\n",
      "Start epoch: 16\n",
      "[16,  1278] loss: 0.0006870\n",
      "Accuracy: 71.804341%\n",
      "Start epoch: 17\n"
     ]
    }
   ],
   "source": [
    "model = CNN(embeddings=glove_embeddings, num_outputs=len(label_to_idx), kernel_num=100, kernel_sizes=[3,4,5], static=False)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, mode='min', verbose=True)\n",
    "\n",
    "train(model, trainloader, testloader, optimizer=optimizer, scheduler=scheduler, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/cnn_google_dropout_0.5_oversample.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
